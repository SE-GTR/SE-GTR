llm:
  base_url: "http://127.0.0.1:8000/v1"
  api_key: "changeme"
  model: "openai/gpt-oss-120b"
  temperature: 0.2
  top_p: 0.9
  max_tokens: 2048
  request_timeout_sec: 300

smelly:
  # If you use the included evidence-enabled Smelly fork, these relative paths work when
  # running from the repository root.
  jar: "tools/backup-smelly-evidence/target/smelly-1.0-shaded.jar"
  evosuite_runtime_jar: "tools/backup-smelly-evidence/evosuite-standalone-runtime-1.2.0.jar"
  junit_jar: "tools/backup-smelly-evidence/junit-4.11.jar"
  detectors: 0
  mode: 0
  sufix: " "

ant:
  ant_cmd: "ant"
  targets_compile: ["clean", "compile", "compile-evosuite"]
  targets_test: []
  shared_lib_dir: "/PATH/TO/REPO/tools/minlib"

  # Optional but recommended for SF110: copy jars from a shared lib dir into each project lib/.
  # shared_lib_dir: "/PATH/TO/ISSTA2026/sf110_projects/lib"
  # Optional: provide an explicit hamcrest jar for JUnitCore validity gate
  # hamcrest_jar: "/path/to/hamcrest-core-1.3.jar"

repair:
  allow_reflection_asserts: false
  max_llm_attempts: 3
  enable_deterministic_rules: true
  limit_tests: 0
  # Context shaping
  cut_context_mode: "signature"   # "signature" or "full"
  cut_context_max_chars: 12000
  cut_signature_include_fields: true
  cut_signature_max_methods: 80
  # Prompt length caps
  max_smell_guides_chars: 12000
  max_evidence_chars: 8000
  max_test_method_chars: 8000
  max_cut_context_chars: 12000
  max_compile_error_chars: 4000
  # Evidence compaction
  evidence_max_list_items: 6
  evidence_max_group_tests: 10
  evidence_max_prefix_stmts: 2
  evidence_max_str_len: 240

logging:
  verbose: true
